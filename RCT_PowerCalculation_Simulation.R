# Longitudinal RCT power calculation
# Goal: Run an a-priori calculation to detect required sample size

# Install packages
install.packages("SimEngine")

# Load libraries
library("SimEngine")

# Simulate dataset & write function
sim <- new_sim()

create_rct_data <- function(n, mu_0, mu_1, sigma_0, sigma_1) {
  group <- sample(rep(c(0,1),n))
  outcome <- (1-group) * rnorm(n=n, mean=mu_0, sd=sigma_0) +
    group * rnorm(n=n, mean=mu_1, sd=sigma_1)
  return(data.frame("group"=group, "outcome"=outcome))
}

# Test the data-generating function
create_rct_data(n=15, mu_0=4, mu_1=3, sigma_0=0.1, sigma_1=0.1)
# NOTE: (1) N = 15 based on best practice recommendations for pilot studies
#       (2) Kenny's MPA Scale ranges from 0 ("strongly disagree") to 6 ("strongly agree") - higher scores reflect severe MPA
## Results of Test:
#ID   group  outcome       # 2 groups (0 = control group; 1 = treatment group)
#1      0   3.954243
#2      0   4.022041
#3      0   4.064514
#4      0   4.046516
#5      1   3.051093
#6      0   4.003851
#7      1   2.985249
#8      1   2.923727
#9      1   2.820070
#10     0   3.946340
#11     0   3.882528
#12     1   2.981958
#13     0   4.027763
#14     0   3.825938
#15     0   4.004236
#16     0   3.954243
#17     1   3.048157
#18     1   2.765523
#19     0   4.046516
#20     0   4.139728
#21     1   2.880314
#22     1   2.985249
#23     1   2.923727
#24     1   2.820070
#25     1   2.992320
#26     0   3.882528
#27     1   2.981958
#28     0   4.027763
#29     1   3.068740
#30     1   2.828176

# Add a function that takes a dataset generated by the function & run a statistical test to decide whether to reject the null hypothesis:
run_test <- function(data) {
  test_result <- t.test(outcome~group, data=data)
  return(as.integer(test_result$p.value<0.05))
}

# Next, write the simulation script & tell SimEngine to run 1.000 simulation replicates each for 4 different sample sizes
sim %<>% set_script(function() {
  data <- create_rct_data(n=L$n, mu_0=4, mu_1=3, sigma_0=2, sigma_1=2)
  reject <- run_test(data)
  return (list("reject"=reject))
})

sim %<>% set_levels(n=c(30,60,90,120))
sim %<>% set_config(num_sim=1000)

# We are now ready to run the simulation. After obtaining results, we calculate power by averaging the ‘reject’ variable using the summarize function, which tells us the percentage of simulations in which the null hypothesis was rejected.

sim %<>% run()
#> Done. No errors or warnings detected. -> Great, we can proceed :)

power_sim <- sim %>% summarize(
  list(stat="mean", name="power", x="reject")
)
print(power_sim)
#level_id    n  n_reps power
#1        1  30   1000 0.481
#2        2  60   1000 0.734
#3        3  90   1000 0.866
#4        4 120   1000 0.935      -> A power of 93.5%, hence we should aim for a sample size around roughly N = 120

# Compare the results to what we obtain by using the power formula
power_formula <- sapply(c(30,60,90,120), function(n) {
  pnorm(sqrt((n*(4-3)^2)/(2^2+2^2)) - qnorm(0.025, lower.tail=F)) # two-sided alpha of 0.05, each tail having 0.025
})

# Visualise the comparison
library(ggplot2)
ggplot(data.frame(
  n = rep(c(30,60,90,120), 2),
  power = c(power_sim$power, power_formula),
  which = rep(c("Simulation","Formula"), each=4)
), aes(x=n, y=power, color=factor(which))) +
  geom_line() +
  labs(color="Method", y="Power", x="Sample size (per group)")

